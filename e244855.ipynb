{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFnkN_Tofg-S",
        "outputId": "00533fd1-2b04-4ad8-fb4f-c442fc7e7333"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Okul\\CV\\THE3\\hw3utils.py:65: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  if save_path is not '':\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "# This cell should be runned. Otherwise, the following cells will not work properly.\n",
        "batch_size = 16\n",
        "max_num_epoch = 100\n",
        "\n",
        "# --- imports ---\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import hw3utils\n",
        "\n",
        "# ---- options ----\n",
        "DEVICE_ID = \"cuda\" if torch.cuda.is_available() else 'cpu' # set to 'cpu' for cpu, 'cuda' / 'cuda:0' or similar for gpu.\n",
        "LOG_DIR = 'checkpoints'\n",
        "VISUALIZE = False # set True to visualize input, prediction and the output from the last batch\n",
        "\n",
        "torch.multiprocessing.set_start_method('spawn', force=True)\n",
        "torch.manual_seed(483) #can be removed if seed is not needed\n",
        "np.random.seed(483) #can be removed if seed is not needed\n",
        "# ---- utility functions -----\n",
        "def get_loaders(batch_size,device):\n",
        "    data_root = 'dataset/ceng483-hw3-dataset'\n",
        "    train_set = hw3utils.HW3ImageFolder(root=os.path.join(data_root,'train'),device=device)\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_set = hw3utils.HW3ImageFolder(root=os.path.join(data_root,'val'),device=device)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    # Note: you may later add test_loader to here.\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# ---- ConvNet -----\n",
        "class Net(nn.Module):\n",
        "    kernel_size = 3\n",
        "\n",
        "    def __init__(self,num_of_layer,num_of_kernel,batch_norm=False,use_tanh=False):\n",
        "        super(Net, self).__init__()\n",
        "        self.num_of_layer = num_of_layer\n",
        "        self.num_of_kernel = num_of_kernel if num_of_layer != 1 else 3\n",
        "        self.batch_norm = batch_norm\n",
        "        self.use_tanh = use_tanh\n",
        "        \n",
        "        self.conv_first = nn.Conv2d(1, self.num_of_kernel, self.kernel_size, padding=1)\n",
        "        self.conv_inter = nn.Conv2d(self.num_of_kernel, self.num_of_kernel, self.kernel_size, padding=1)\n",
        "        self.conv_last = nn.Conv2d(self.num_of_kernel, 3, self.kernel_size, padding=1)\n",
        "        self.batch_norm_inter = nn.BatchNorm2d(self.num_of_kernel)\n",
        "        self.batch_norm_out = nn.BatchNorm2d(3)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, grayscale_image):\n",
        "        # apply your network's layers in the following lines:\n",
        "        x = None\n",
        "        for layer_i in range(self.num_of_layer):\n",
        "            if layer_i == 0:\n",
        "                x = self.conv_first(grayscale_image)\n",
        "                if self.batch_norm: x = self.batch_norm_inter(x)\n",
        "                if layer_i != self.num_of_layer-1:  x = self.relu(x)\n",
        "                \n",
        "            elif layer_i == self.num_of_layer-1:\n",
        "                x = self.conv_last(x)\n",
        "                if self.batch_norm: x = self.batch_norm_out(x)\n",
        "            else:\n",
        "                x = self.conv_inter(x)\n",
        "                if self.batch_norm: x = self.batch_norm_inter(x)\n",
        "                x = self.relu(x)\n",
        "        if self.use_tanh: x = self.tanh(x)\n",
        "        return x\n",
        "\n",
        "# ---- MarginLoss -----\n",
        "class MarginLoss(nn.Module):\n",
        "    def __init__(self, margin):\n",
        "        super(MarginLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        \n",
        "    def forward(self, predictions, true_values):\n",
        "        \n",
        "        loss = torch.abs(true_values-predictions)\n",
        "        loss = torch.maximum(loss - self.margin,torch.zeros_like(loss))         \n",
        "        loss = torch.mean(torch.square(loss))\n",
        "        \n",
        "        return loss\n",
        "\n",
        "# ---- training code -----\n",
        "device = torch.device(DEVICE_ID)\n",
        "print('device: ' + str(device))\n",
        "\n",
        "criterion_mse = nn.MSELoss()\n",
        "criterion_margin = MarginLoss(24/255)\n",
        "train_loader, val_loader = get_loaders(batch_size,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This functions are called inside the train() function, but they should be uncommented and runned if you want to get the results.\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_losses(train_losses, val_losses,title):\n",
        "    \"\"\"Plots the losses for training and validation sets with respect to epochs.\"\"\"\n",
        "    plt.close()\n",
        "    last_point_tr = train_losses[-1]\n",
        "    last_point_val = val_losses[-1]\n",
        "    last_point_x = len(train_losses)-1\n",
        "    \n",
        "    plt.scatter(last_point_x, last_point_tr, color='blue', label=f\"Loss:{last_point_tr:.4f} \")\n",
        "    plt.scatter(last_point_x, last_point_val, color='red', label=f\"Loss:{last_point_val:.4f} \")\n",
        "    \n",
        "    plt.plot(train_losses, 'b', label='Training loss')\n",
        "    plt.plot( val_losses, 'r', label='Validation loss')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.savefig(LOG_DIR+\"/\"+title+\"_plot.jpg\")\n",
        "    plt.close()\n",
        "\n",
        "def visualize_same_batch(net,title):\n",
        "    \"\"\"Visualizes the input, prediction and the output from the first 6 batches.\n",
        "\n",
        "    Args:\n",
        "        net: model\n",
        "        title (str): name of the saved file \n",
        "    \"\"\"\n",
        "    for iteri, data in enumerate(val_loader, 0):\n",
        "        if iteri == 6:\n",
        "            break\n",
        "        inputs, targets = data\n",
        "        preds = net(inputs)\n",
        "        hw3utils.visualize_batch(inputs[:8],preds[:8],targets[:8],os.path.join(LOG_DIR,f'{title}_img'+str(iteri)+'.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_loss(loader,model,loss_func):\n",
        "    \"\"\"returns the average loss on the given data set and over the given model.\n",
        "\n",
        "    Args:\n",
        "        loader: data loader\n",
        "        model: trained model\n",
        "        loss_func (function): function that calculates the loss\n",
        "\n",
        "    Returns:\n",
        "        float: average loss\n",
        "    \"\"\"\n",
        "    total_loss = 0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        inputs, labels = data\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss/len(loader)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ouK1b5saho_4"
      },
      "outputs": [],
      "source": [
        "#This cell should be runned if you want to train the model. Otherwise, no need.\n",
        "early_stop_delta = 0.0001\n",
        "early_stop_counter_max = 3\n",
        "\n",
        "\n",
        "def train(num_of_layer,num_of_kernel,lr,batch_norm,use_tanh):\n",
        "    \"\"\"Trains the model with the given parameters.\n",
        "        Used loss function in training is MSE.\n",
        "        Used loss function in validation is 12-Margin Error.\n",
        "    Args:\n",
        "        num_of_layer (int): number of layers\n",
        "        num_of_kernel (int): number of kernels\n",
        "        lr (float): learning rate\n",
        "        batch_norm (bool): whether to use batch normalization\n",
        "        use_tanh (bool): whether to use tanh \n",
        "    \"\"\"\n",
        "    title = f\"Layer{num_of_layer}_Kernel{num_of_kernel}_LR{lr}_BN{batch_norm}_TANH{use_tanh}\"\n",
        "    net = Net(num_of_layer=num_of_layer,num_of_kernel = num_of_kernel,batch_norm=batch_norm,use_tanh=use_tanh).to(device=device)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "    print(f'training begins --> {num_of_layer} layers | {num_of_kernel} kernels | lr = {lr} | batch_norm = {batch_norm} | use_tanh = {use_tanh}')\n",
        "    loss_lst_valid = []\n",
        "    loss_lst_train = []\n",
        "    early_stop_counter = 0\n",
        "    min_validation_loss = float('inf')\n",
        "    for epoch in range(max_num_epoch):\n",
        "        for iteri, data in enumerate(train_loader, 0):\n",
        "            inputs, targets = data\n",
        "            optimizer.zero_grad() # zero the parameter gradients\n",
        "            # do forward, backward, SGD step\n",
        "            preds = net(inputs)\n",
        "            loss = criterion_mse(preds, targets)\n",
        "            # loss = criterion_margin(preds, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()  \n",
        "            print(f\"Batch: {iteri}/{len(train_loader)}\", end='\\r')\n",
        "        \n",
        "        avg_loss_train = calculate_loss(train_loader,net,criterion_mse)\n",
        "        avg_loss_val = calculate_loss(val_loader,net,criterion_margin)\n",
        "        \n",
        "        \n",
        "\n",
        "        print('Epoch %d training loss: %.5f' %\n",
        "                (epoch + 1, avg_loss_train))\n",
        "        print('Epoch %d validation loss: %.5f' %\n",
        "                (epoch + 1, avg_loss_val))\n",
        "        \n",
        "        #Early stop\n",
        "        if (min_validation_loss - avg_loss_val ) < early_stop_delta :\n",
        "            early_stop_counter += 1\n",
        "            print(f\"Early_stop counter {early_stop_counter}\")\n",
        "            if early_stop_counter >= early_stop_counter_max:\n",
        "                print(f\"Early_stop with last epoch {avg_loss_val}\")\n",
        "                break\n",
        "        else:\n",
        "            early_stop_counter = 0\n",
        "        min_validation_loss = min(avg_loss_val,min_validation_loss)\n",
        "                \n",
        "        loss_lst_valid.append(avg_loss_val)\n",
        "        loss_lst_train.append(avg_loss_train)\n",
        "                \n",
        "        print('Saving the model, end of epoch %d' % (epoch+1))\n",
        "        if not os.path.exists(LOG_DIR):\n",
        "            os.makedirs(LOG_DIR)    \n",
        "        torch.save(net.state_dict(), os.path.join(LOG_DIR,f'{title}.pt'))\n",
        "        # visualize_same_batch(net, title) #uncomment if you want to visualize and save the input, prediction and the output from the first 6 batches.\n",
        "        # plot_losses(loss_lst_train,loss_lst_valid,title) #uncomment if you want to plot the losses.\n",
        "    print('Finished Training')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL1NK0NCxYPM",
        "outputId": "96c60397-4bf2-49ba-9f5e-2c1a5966a5b9"
      },
      "outputs": [],
      "source": [
        "#THIS CELL SHOULD NOT BE RUNNED UNLESS YOU WANT TO GRID SEARCH THE HYPERPARAMETERS\n",
        "#Used for baseline model Part 1\n",
        "num_of_layers = [1,2,4]\n",
        "num_of_kernels = [2,4,8]\n",
        "learning_rates = [0.0001,0.001,0.01,0.1]\n",
        "\n",
        "for num_of_layer in num_of_layers:\n",
        "        for num_of_kernel in num_of_kernels:\n",
        "            for lr in learning_rates:\n",
        "                train(num_of_layer,num_of_kernel,lr,False,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "#This cell should be runned if you want to create the npy file.\n",
        "#It will directly create a npy file when runned.\n",
        "import hw3utils\n",
        "saved_model_file_name = \"Layer2_Kernel8_LR0.001_BNFalse_TANHTrue\" #change this to the name of the model you want to use\n",
        "\n",
        "def get_test_loader():\n",
        "    \"\"\"Returns the test loader for test images\"\"\"\n",
        "    test_root = 'test' # change to the directory of test images\n",
        "    test_set = hw3utils.HW3ImageFolder(root=os.path.join(test_root),device=device)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
        "    return test_loader\n",
        "\n",
        "    \n",
        "def create_npy(title):\n",
        "    \"\"\"Creates an estimation npy file for test images\n",
        "\n",
        "    Args:\n",
        "        title (str): name of the pt file to be loaded as model withot .pt\n",
        "    \"\"\"\n",
        "    directory = LOG_DIR # change to the directory of the model\n",
        "    test_loader = get_test_loader()\n",
        "    # _,val_load =  get_loaders(1,device) #used for calculating accuracy on validation set\n",
        "    \n",
        "    results_array = np.zeros((100, 80, 80, 3), dtype=np.float32)\n",
        "    saved_state_dict = torch.load(directory+\"/\"+title+'.pt')\n",
        "    \n",
        "    #change the parameters according to the loaded model\n",
        "    net2 = Net(num_of_layer=2,num_of_kernel = 8,batch_norm=False,use_tanh=True).to(device=device)  #Parameters of my best model\n",
        "    net2.load_state_dict(saved_state_dict)\n",
        "    myfile = open('test_images.txt', 'w')\n",
        "    myfile.write(\"\") #clearing inside if it is not empty\n",
        "    # Iterate through the test set\n",
        "    for iteri, data in enumerate(test_loader, 0):\n",
        "        \n",
        "        inputs, _ = data\n",
        "        preds = net2(inputs)\n",
        "        preds_array = preds[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
        "        \n",
        "        results_array[iteri] = preds_array\n",
        "        if iteri == 99:\n",
        "            break\n",
        "\n",
        "    # Convert the results array to [0,255] from [-1,1]\n",
        "    results_array = ((results_array+1) * (255/2)).astype(np.uint8)\n",
        "    print(len(results_array)) #should be 100\n",
        "    \n",
        "    # Save the results array to a file\n",
        "    np.save('estimations_test.npy', results_array)\n",
        "\n",
        "create_npy(saved_model_file_name) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python evaluate.py estimations.npy img_names.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
